{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Modeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm as tqdma\n",
    "from datasets.create_dataset import create_dataset\n",
    "from models.create_model import create_model\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(vec2sort: np.ndarray, *data_: tuple[np.ndarray,...]):\n",
    "    sort_ids = np.argsort(vec2sort)\n",
    "    sorted_data_ = [None] * len(data_)\n",
    "    for i, data in enumerate(data_):\n",
    "        sorted_data_[i] = np.zeros_like(data)\n",
    "        if len(data.shape)>1:\n",
    "            for j in range(data.shape[1]):\n",
    "                sorted_data_[i][:,j] = data[sort_ids,j]\n",
    "        else:\n",
    "            sorted_data_[i] = data[sort_ids]\n",
    "    if len(data_) > 1:\n",
    "        return tuple(sorted_data_), sort_ids\n",
    "    else:\n",
    "        return sorted_data_[0], sort_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config: argparse.Namespace):\n",
    "    torch.manual_seed(42)\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    # Create dataset\n",
    "    phys_config = {\n",
    "        'n_dof' : config.n_dof,\n",
    "        'system-type' : config.system_type\n",
    "    }\n",
    "    data_parameters = {\n",
    "        'sequence_length' : config.sequence_length,\n",
    "        'subsample' : config.subsample,\n",
    "        'downsample' : config.downsample\n",
    "    }\n",
    "    phases = ['train', 'val', 'test']\n",
    "    full_dataset = create_dataset(phys_config, data_parameters)\n",
    "    train_size = 1.0\n",
    "    val_size = 0.0\n",
    "    test_size = 0.0\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])\n",
    "    datasets = {\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_dataset\n",
    "    }\n",
    "    dataloaders = {\n",
    "        x: DataLoader(dataset=datasets[x], batch_size=config.batch_size, shuffle=True if x == 'train' else False, num_workers=config.num_workers, pin_memory=True) for x in phases}\n",
    "\n",
    "    # Create pinn model\n",
    "    if config.out_channels != 2*config.n_dof:\n",
    "        raise Exception(\"Number of network outputs does not match state vector of simulated model\")\n",
    "    pinn_config = {\n",
    "        'n_dof' : config.n_dof,\n",
    "        'phys_system_type' : config.phys_system_type,\n",
    "        'system_discovery' : config.system_discovery,\n",
    "        'm_' : config.m_,\n",
    "        'c_' : config.c_,\n",
    "        'k_' : config.k_,\n",
    "        'kn_' : config.kn_,\n",
    "        'alphas' : full_dataset.alphas,\n",
    "        'param_norms' : {\n",
    "            'm' : 1.0,\n",
    "            'c' : 1.0,\n",
    "            'k' : 10.0,\n",
    "            'kn' : 10.0\n",
    "        }\n",
    "    }\n",
    "    model = create_model(config.model_type, config.in_channels, config.latent_features, config.out_channels, config.sequence_length, pinn_config)\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay, amsgrad=True)\n",
    "    model.set_switches(config.lambdas)\n",
    "\n",
    "    # set up plotting and printing\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(12,12))\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    loss_hist = []\n",
    "    loss_hist_sub = []\n",
    "    print_step = 1000\n",
    "    num_obs_samps = len(datasets['train']) * config.sequence_length\n",
    "    num_col_samps = len(datasets['train']) * config.subsample * config.sequence_length\n",
    "    t_span_obs = torch.zeros((num_obs_samps)).numpy()  # time vector for observation domain\n",
    "    obs_state = torch.zeros((num_obs_samps, 2 * config.n_dof))  # all state observations\n",
    "    t_span_gt = torch.zeros((num_col_samps)).numpy()  # time vector for gt/prediction domain\n",
    "    ground_truth = torch.zeros((num_col_samps, 3 * config.n_dof + 1)).numpy()  # all data for gound truth (state, time, force)\n",
    "    predictions = torch.zeros((num_col_samps, 2 * config.n_dof)).numpy()  # all state predictions (same as collocation domain)\n",
    "\n",
    "    # Training loop\n",
    "    epoch = 0\n",
    "    model = model.to(device)\n",
    "    progress_bar = tqdm(total=config.num_epochs)\n",
    "    lambdas = config.lambdas\n",
    "    while epoch < config.num_epochs:\n",
    "        write_string = ''\n",
    "        write_string += 'Epoch {}\\n'.format(epoch)\n",
    "        if (epoch+1) % 10000 == 0 and epoch+1 < 45000:\n",
    "            lambdas['ode'] *= 5\n",
    "        for phase in phases:\n",
    "            phase_loss = 0.\n",
    "            losses = [0.0] * 4\n",
    "            write_string += '\\tPhase {}\\n'.format(phase)\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            for i, (sample, coll_data) in enumerate(dataloaders[phase]):\n",
    "                # parse data sample\n",
    "                state = sample[..., :2*config.n_dof].to(device).float().requires_grad_()\n",
    "                t_span = sample[..., 2*config.n_dof].to(device).float().requires_grad_()\n",
    "                t_coll = coll_data[..., 2*config.n_dof].to(device).float().requires_grad_()\n",
    "                force = coll_data[..., 2*config.n_dof+1:].to(device).float().requires_grad_()\n",
    "\n",
    "                inputs = t_span\n",
    "                targets = state\n",
    "                t_coll = t_coll.reshape(-1,config.sequence_length)  # unrolls collocation data\n",
    "                force = force.reshape(-1,config.sequence_length,config.n_dof)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                loss, losses_i, _ = model.loss_func(lambdas, inputs, targets, t_coll, force, epoch)\n",
    "                phase_loss += loss.item()\n",
    "                losses = [losses[j] + loss_i for j, loss_i in enumerate(losses_i)]\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            if phase == 'train':\n",
    "                loss_hist.append([loss_it.item() for loss_it in losses] + [phase_loss])\n",
    "            write_string += '\\tLoss {}\\n'.format(phase_loss)\n",
    "        if config.system_discovery:\n",
    "            write_string += '\\tSystem Parameters:\\tc - {:.4f} [{:.2f}]\\tk - {:.4f} [{:.2f}]\\tkn - {:.4f} [{:.2f}]\\n'.format(\n",
    "                model.c_[0,0].item()*pinn_config['param_norms']['c'],\n",
    "                config.c_[0,0].item(),\n",
    "                model.k_[0,0].item()*pinn_config['param_norms']['k'],\n",
    "                config.k_[0,0].item(),\n",
    "                model.kn_[0,0].item()*pinn_config['param_norms']['kn'],\n",
    "                config.kn_[0,0].item())\n",
    "        \n",
    "        if (epoch+1) % print_step == 0:\n",
    "\n",
    "            for i, (sample, coll_data) in enumerate(dataloaders['train']):\n",
    "                inpoint_obs = i * config.batch_size * config.sequence_length\n",
    "                outpoint_obs = (i+1) * config.batch_size * config.sequence_length\n",
    "                inpoint_col = i * config.batch_size * config.subsample * config.sequence_length\n",
    "                outpoint_col = (i+1) * config.batch_size * config.subsample * config.sequence_length\n",
    "\n",
    "                obs_inputs = sample[..., 2 * config.n_dof].to(device).float()\n",
    "                t_span_obs[inpoint_obs:outpoint_obs] = obs_inputs.numpy().reshape(-1)\n",
    "                obs_state[inpoint_obs:outpoint_obs,:] = sample[..., :2 * config.n_dof].float().reshape(-1,2*config.n_dof)\n",
    "\n",
    "                t_coll = coll_data[..., 2 * config.n_dof].to(device).float().requires_grad_()\n",
    "                pred_inputs = t_coll.reshape(-1, config.sequence_length)\n",
    "\n",
    "                ground_truth[inpoint_col:outpoint_col,:] = coll_data.reshape(-1, 3 * config.n_dof + 1).cpu().numpy()\n",
    "                t_span_gt[inpoint_col:outpoint_col] = pred_inputs.reshape(-1).detach().cpu().numpy()\n",
    "                predictions[inpoint_col:outpoint_col, :] = model(pred_inputs).detach().cpu().reshape(-1, 2 * config.n_dof).numpy()\n",
    "\n",
    "            (ground_truth, predictions, t_span_gt), _ = sort_data(t_span_gt, ground_truth, predictions, t_span_gt)\n",
    "            (obs_state, t_span_obs), _ = sort_data(t_span_obs, obs_state, t_span_obs)\n",
    "\n",
    "            for kinetic in range(2):\n",
    "                axs[kinetic].cla()\n",
    "                axs[kinetic].plot(t_span_gt * model.alpha_t.item(),\n",
    "                                  ground_truth[:, kinetic] * model.alpha_z[kinetic].item(),\n",
    "                                  label='Exact Solution', color='tab:blue', linewidth=1.0)\n",
    "                axs[kinetic].plot(t_span_gt * model.alpha_t.item(),\n",
    "                                  predictions[:, kinetic] * model.alpha_z[kinetic].item(),\n",
    "                                  label='Prediction', linestyle='--', color='tab:red', linewidth=1.0)\n",
    "                # axs[kinetic].plot(t_span_obs * model.alpha_t.item(),\n",
    "                #                   obs_state[:, kinetic] * model.alpha_z[kinetic].item(),\n",
    "                #                   label='Observation Data', linestyle='None', marker='o',\n",
    "                #                   color='tab:gray', markersize=4)\n",
    "                axs[kinetic].legend()\n",
    "\n",
    "            loss_vec = torch.arange(epoch+1)\n",
    "            loss_step = len(loss_hist) // int(10e3) + 1\n",
    "            loss_vec = loss_vec[::loss_step]\n",
    "            loss_hist_sub = loss_hist_sub + loss_hist[::loss_step]\n",
    "            loss_hist = []\n",
    "            axs[2].cla()\n",
    "            axs[2].plot(loss_vec,loss_hist_sub)\n",
    "            axs[2].legend(['obs', 'cc', 'ode', 'ic', 'total'])\n",
    "            axs[2].set_yscale('log')\n",
    "\n",
    "            dh.update(fig)\n",
    "            tqdma.write(write_string)\n",
    "            \n",
    "        epoch += 1\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse config\n",
    "parser = argparse.ArgumentParser(description=\"Train PINN\")\n",
    "\n",
    "# physical-model arguments\n",
    "parser.add_argument('--n-dof', type=int, default=1)\n",
    "parser.add_argument('--system-type', type=str, default='single_dof_duffing')\n",
    "parser.add_argument('--subsample', type=int, default=1)\n",
    "parser.add_argument('--downsample', type=int, default=1)\n",
    "\n",
    "# nn-model arguments\n",
    "parser.add_argument('--model-type', type=str, default='sdof_pinn')\n",
    "parser.add_argument('--in-channels', type=int, default=1)\n",
    "parser.add_argument('--latent-features', type=int, default=32)\n",
    "parser.add_argument('--out-channels', type=int, default=2)\n",
    "\n",
    "# pinn arguments\n",
    "parser.add_argument('--phys-system-type', type=str, default='duffing_sdof')\n",
    "parser.add_argument('--lambdas', type=dict, default={\n",
    "    'obs' : 0.0,\n",
    "    'cc' : 0.0,\n",
    "    'ode' : 100.0,\n",
    "    'ic' : 1.0\n",
    "})\n",
    "parser.add_argument('--system-discovery', type=bool, default=False)\n",
    "parser.add_argument('--m-', type=torch.Tensor, default=torch.Tensor([[10.0]]))\n",
    "parser.add_argument('--c-', type=torch.Tensor, default=torch.Tensor([[1.0]]))\n",
    "parser.add_argument('--k-', type=torch.Tensor, default=torch.Tensor([[15.0]]))\n",
    "parser.add_argument('--kn-', type=torch.Tensor, default=torch.Tensor([[100.0]]))\n",
    "\n",
    "# training arguments\n",
    "parser.add_argument('--task', type=str, default=\"instance\")\n",
    "parser.add_argument('--batch-size', type=int, default=16)\n",
    "parser.add_argument('--num-workers', type=int, default=0)\n",
    "parser.add_argument('--num-epochs', type=int, default=500000)\n",
    "parser.add_argument('--sequence-length', type=int, default=1)\n",
    "parser.add_argument('--learning-rate', type=float, default=1e-3)\n",
    "parser.add_argument('--weight-decay', type=float, default=0)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "model = main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
